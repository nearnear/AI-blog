{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/DL/regularization/","result":{"data":{"allMarkdownRemark":{"totalCount":58},"markdownRemark":{"id":"8437c672-3d73-545c-bd4f-15df29c6c3c0","html":"<h2>Regularization</h2>\n<p>Regularization이란 학습 데이터셋에 overfitting 되는 것을 지양하고 테스트 데이터셋에 대한 loss를 줄이는 것을 의미한다. 즉 모델의 <strong>Variance를 줄이는 것</strong> 이 목표이며, 특정 데이터나 데이터셋에 특화되지 않고 타겟으로하는 데이터에 일반적으로 적용될 수 있는 파라미터를 학습하는 것이 목표이다. Regularization은 특히 해석적 기반이 있기 보다 <code>경험적</code>으로 검증된 방식이 많다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA0ElEQVR42l2QC47DIAxEe/97NmxSPgaMgYRkVribSl1LFtLAPOx5XNeFWb13jPNESglFimrHceAYA7U3lFpVm+/7vkNEkIjAUnCe5+fucQO985BS0aShsqgmUuDNip0ixHqMzNMGu26oLqB60lbYf6CzTpuZEXxAikmn/lkMcoyIPsC9LMYYeC0GZB0ys36YKX5PeENrrfDBI+WkxjsKihEUCa131eaKmTOIgq48o8If5wvInGGMgbUWd82sjFnwXJ5orX2AIQRs26rn2/8G/gIK9YTZgHFDfwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"bias variance tradeoff\"\n        title=\"\"\n        src=\"/static/afa27438996a71f0ed2601fffc1d9c30/c1b63/bias-variance-tradeoff.png\"\n        srcset=\"/static/afa27438996a71f0ed2601fffc1d9c30/5a46d/bias-variance-tradeoff.png 300w,\n/static/afa27438996a71f0ed2601fffc1d9c30/0a47e/bias-variance-tradeoff.png 600w,\n/static/afa27438996a71f0ed2601fffc1d9c30/c1b63/bias-variance-tradeoff.png 1200w,\n/static/afa27438996a71f0ed2601fffc1d9c30/5fada/bias-variance-tradeoff.png 1706w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>image from <a href=\"https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\">towardsdatascience.com</a></p>\n<br>\n<h3>1. Early Stopping</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA/0lEQVR42nWRjW7DIAyE+/4vuHVbtzVZlDZhDcRAEyD5JpBaaf05yfgQtrmDDcCyLJhRWNcVWEu+jYxH/LZmcyHbty9iTDxDaboZ+qhmczl73VY4P0GKBGNIMhKNJjl7pyIjpcQ8z4QQiDEWl0VhXmIMVFVD3ZxYJ09SHX77wlRXRKX+qQghIiJobfDe45zjfD6XC66W80ZkRJuJuhnwCWT3QTC62Ewx4L3DWsE5Swzzc8sXopRinies9dQ/it17w+eu5Xvfsa862oPm0Am98vS/nq63DNoidi5x9ylZeraR7XhnsdZhjNC2R/peFa71yKBNyafTgEh+X0jLch34BzJLIfDQ91tXAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"early stopping\"\n        title=\"\"\n        src=\"/static/edfb8bf28b352505b5d28efa4d737b8b/c1b63/early-stopping.png\"\n        srcset=\"/static/edfb8bf28b352505b5d28efa4d737b8b/5a46d/early-stopping.png 300w,\n/static/edfb8bf28b352505b5d28efa4d737b8b/0a47e/early-stopping.png 600w,\n/static/edfb8bf28b352505b5d28efa4d737b8b/c1b63/early-stopping.png 1200w,\n/static/edfb8bf28b352505b5d28efa4d737b8b/27f8b/early-stopping.png 1730w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>image from boostcourse.org, 딥러닝 기초 다지기</p>\n<p>Early Stopping은 validation 데이터셋에 대한 loss가 증가하기 전에 학습을 멈추는 전략이다. 일반적으로 학습이 경과함에 따라 training 데이터셋에 대한 loss는 완만하게 감소하는 반면 validation 데이터셋에 대한 loss는 어느 지점에서부터 상승할 가능성이 크다. 이는 모델의 일반화 성능이 감소하기 시작한다는 의미로 해석할 수 있다. 이때 validation은 training 데이터셋과 중복되지 않는 데이터셋이어야 하며, 이러한 검증이 validation 데이터셋을 생성하는 의미이다.</p>\n<br>\n<h3>2. Parameter Norm Penalty</h3>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>total cost</mtext><mo>=</mo><mtext>loss</mtext><mo stretchy=\"false\">(</mo><mi mathvariant=\"script\">D</mi><mo separator=\"true\">;</mo><mi>W</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mfrac><mi>α</mi><mn>2</mn></mfrac><msubsup><mrow><mo stretchy=\"false\">∥</mo><mi>W</mi><mo stretchy=\"false\">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\text{total cost} = \\text{loss} (\\mathcal{D}; W) + \\frac{\\alpha}{2} {\\lVert W \\rVert}^2_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord text\"><span class=\"mord\">total cost</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">loss</span></span><span class=\"mopen\">(</span><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.7936em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1076em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen\">∥</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mclose\">∥</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.954em;\"><span style=\"top:-2.4003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span></span></span></span></span></div>\n<p>우변의 둘째항은 function space의 smoothing을 담당하는 Parameter Norm Penalty이다. 파라미터에 대한 Norm(L2의 경우 벡터의 크기)를 더함으로서 전체 비용 함수를 더 부드럽게(smooth)할 수 있는데, penalty를 더하지 않은 함수보다 일반화 성능이 더 높을 것이라는 것을 가정으로 한 전략이다.</p>\n<br>\n<h3>3. Data Augmentation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.666666666666668%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA30lEQVR42mWRiY7DIAxE+//fWWnVNC23bcwxFV4lm2qRLGzLPGbgBgBzToQQICI41miKGDxyzlbXWhFjRO8dx5ljv+a3o0gpobVmeecETQ6VsoGs17vNXGGtDxDL1yUnUIQxxkSXjK6C1SUppvpQp6qnA64KWsH0H7iCWTB7w1Q2GFdCpoLWugG99xD5VVsooVAGlzf25w+Y+Q+46AYkQvQPxBzgUkTIBVUbxhjnMMUAv93h3k80KVAhaJWvmdOyaoVbn1DI1C6rr9d+vtuSLVSwbQ8451HNLmPfd4yL5Q+IctYyInYQAgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"dataset scale\"\n        title=\"\"\n        src=\"/static/54cbb5d196f713121177c8fa48c46e35/c1b63/dataset-scale.png\"\n        srcset=\"/static/54cbb5d196f713121177c8fa48c46e35/5a46d/dataset-scale.png 300w,\n/static/54cbb5d196f713121177c8fa48c46e35/0a47e/dataset-scale.png 600w,\n/static/54cbb5d196f713121177c8fa48c46e35/c1b63/dataset-scale.png 1200w,\n/static/54cbb5d196f713121177c8fa48c46e35/d61c2/dataset-scale.png 1800w,\n/static/54cbb5d196f713121177c8fa48c46e35/aae30/dataset-scale.png 2080w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>image from boostcourse.org, 딥러닝 기초 다지기</p>\n<p>일반적으로 데이터가 많을 수록 딥러닝이 트리 기반 모델보다 더 우수한 성능을 낸다. 즉 복잡한 네트워크 구조를 활용하기 위해서는 많은 데이터가 필요하다. 그렇지만 실질적으로 많은 데이터를 구할 수 없는 경우가 많은데, 이때 라벨이 변하지 않는 한도 내에서 이미 가지고 있는 데이터셋을 변형시키는 것이 Data Augmentation이다.</p>\n<blockquote>\n<p>Fashion MNIST 데이터셋에 대해 Data Augmentation을 수행했을 때 오히려 metric이 악화되는 경험을 하였다. 이때 이미지를 좌우 반전 시키거나 중심을 이동시키는 것으로 라벨이 바뀌는 경우가 아니었다. 이 경우에는 전체 데이터가 워낙 잘 정제되어 있고(좌우 trimming, 사이즈, 각도 등) 따라서 Test 데이터셋의 분산이 Training 데이터셋의 분산과 거의 일치했기 때문에 이러한 결과가 나왔을 것이라고 추측하였다.</p>\n</blockquote>\n<br>\n<h3>4. Noise Robustness</h3>\n<p>Noise Robustness는 Rule of Thumb로 검증된 방식으로, training 데이터셋 and/or 신경망의 가중치에 노이즈를 더하는 방식이다. 즉 학습 과정에서 randomness를 더하는 것으로 일반화 성능을 높일 수 있는 것이다.</p>\n<br>\n<h3>5. Label Smoothing</h3>\n<p><code>분류</code> 문제에 있어서 decision boundary를 완화하는 것을 Label Smoothing이라고 하며, 일반적으로 성능을 향상시킬 수 있는 방법 중의 하나다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 26.666666666666668%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABJklEQVR42nWQv0sCARTH/Scigub21v6AluYG2wrEMFoEf0VRmf2gxZaWhiJ0L84kUdJU7iTjpCIIC1JpKfBwuPN0OO8Td3ZFRG/5vvfgfb7vPRdfYZqmk9JWFGp3Dzw9v1Kvv1AWy1Qfb6lIEteiyNv7B/dyDalSRdX0X/OuwWDwXfR6PVvFyg3bgQUOokHi8Tgbfg+b0VVGRseYnBjn+OiQ9UiQkNeNJJbtGYtjAy2YA+x0OvT7fa7yJULL84QXZ4mEQ2ytBdjfi+Hx+nDPTLHiX2J3J4ZvbppcJj0EGsYQ6JxpOVgwwzBotloIKYHMZZpCIU8ieUpKOEeWZS6EM0rFItlsjmTihGaj8f+GqqraW1qh6/rPT9uKrZaZ0+92NTSt++f/nyI6WF/nPBlUAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"label smoothing comparison\"\n        title=\"\"\n        src=\"/static/ceb957ad058064dfd08d1d916e9d0d0c/c1b63/label-smoothing-comparison.png\"\n        srcset=\"/static/ceb957ad058064dfd08d1d916e9d0d0c/5a46d/label-smoothing-comparison.png 300w,\n/static/ceb957ad058064dfd08d1d916e9d0d0c/0a47e/label-smoothing-comparison.png 600w,\n/static/ceb957ad058064dfd08d1d916e9d0d0c/c1b63/label-smoothing-comparison.png 1200w,\n/static/ceb957ad058064dfd08d1d916e9d0d0c/1e5d2/label-smoothing-comparison.png 1630w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>image from <a href=\"https://arxiv.org/abs/1905.04899\">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, 2019</a></p>\n<p>위의 이미지는 Label Smoothing 방법인 Mixup, Cutout, CutMix를 비교한 것이다. Mixup은 Training 데이터셋을 임의로 두개의 집합으로 나누어 입력값과 라벨을 특정 비율로 섞는 것이고, Cutout은 Training 데이터셋의 입력값을 임의로 잘라내는 방식이다. CutMix는 앞의 두 방법을 더하되 입력값의 특정 영역을 서로 대체하는 방식으로 섞는다.</p>\n<blockquote>\n<p>글을 작성하는 시점의 예측으로는, soft target을 통한 knowledge distillation과 비슷한 이유로 정보가 더해져 성능이 향상되는 것이 아닐까 생각된다.</p>\n</blockquote>\n<br>\n<h3>6. Dropout</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAx0lEQVR42mWQ2wpGQBSFvf+bkdyQC4oLx5yJyGn9rV0mv1Grsfea+fbBAID7vvE+m6ZBVVUq9/W7rpM779zzGd/L27YhyzLRsiwadN93KUbguq6ab3wTcRzDNE04joMwDJV3XZf4SZKICAyCQOteOjzPU4JhGOC6LmzbFvG/73s1znEcErNQXdeIoght2/4DCXsCjuj7PizLEnmeh3meFZDr4LjcIZXnuTTxB+QoTzBNE4qiQJqmskM+GMdRAekzx5OFyrLUgD+kIM25sQf19gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"dropout\"\n        title=\"\"\n        src=\"/static/706558cbbb5be7b902be98f9178abd38/c1b63/dropout.png\"\n        srcset=\"/static/706558cbbb5be7b902be98f9178abd38/5a46d/dropout.png 300w,\n/static/706558cbbb5be7b902be98f9178abd38/0a47e/dropout.png 600w,\n/static/706558cbbb5be7b902be98f9178abd38/c1b63/dropout.png 1200w,\n/static/706558cbbb5be7b902be98f9178abd38/d61c2/dropout.png 1800w,\n/static/706558cbbb5be7b902be98f9178abd38/9ce1c/dropout.png 1886w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>image from boostcourse.org, 딥러닝 기초 다지기</p>\n<p>Dropout은 신경망의 Inference 단계에서 가중치를 특정 비율만큼 0으로 대체하는 방식으로, 특히 깊은 네트워크에서 성능을 향상시키는 주요 방법 중의 하나다.</p>\n<br>\n<h3>7. Batch Normalization</h3>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mtable rowspacing=\"0.25em\" columnalign=\"right left\" columnspacing=\"0em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><msub><mi>μ</mi><mi>B</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><msubsup><mi>σ</mi><mi>B</mi><mn>2</mn></msubsup></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>B</mi></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mover accent=\"true\"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>B</mi></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{aligned}\n\\mu_B &#x26;= \\frac{1}{m} \\sum^m_{i=1} x_i \\\\\n\\sigma^2_B &#x26;= \\frac{1}{m} \\sum^m_{i=1} {(x_i - \\mu_B)}^2 \\\\\n\\hat{x_i} &#x26;= \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n\\end{aligned}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:9.1485em;vertical-align:-4.3242em;\"></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:4.8242em;\"><span style=\"top:-6.8242em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.5952em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-0.7572em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6944em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:4.3242em;\"><span></span></span></span></span></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:4.8242em;\"><span style=\"top:-6.8242em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">m</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.5952em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">m</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.954em;\"><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span><span style=\"top:-0.7572em;\"><span class=\"pstrut\" style=\"height:3.6514em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2603em;\"><span style=\"top:-2.1738em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9362em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7959em;\"><span style=\"top:-2.4065em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span><span style=\"top:-3.0448em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2935em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span><span style=\"top:-2.8962em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.28em\" viewBox=\"0 0 400000 1296\" preserveAspectRatio=\"xMinYMin slice\"><path d=\"M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z\"></path></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3038em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.13em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:4.3242em;\"><span></span></span></span></span></span></span></span></span></span></span></span></div>\n<p>Batch Normalization은 이름 그대로 Layer에서의 Batch 평균과 표준 편차를 계산해 정규화(Normalization)을 수행하는 전략이다. Dropout과 같이 깊은 네트워크의 성능을 향상시키는 주요 방법으로, <a href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization,  2015</a>에서 소개되었다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABNklEQVR42j2Qy0oCARiFp1Vv1tqXaFGLaBvRoh4hLCJatAqMFIPCiMLLwkQUtXC6l5e5OV7GwRnTcWa+cCY78C8OBz7OfwTf95nfXIZhMOzpLORNbDRNw/O8wJumiaqqOI4T+LE1otVqYtt24OccYQHrDwaI9ToX2RLXtSYpUeUgVUR8eqTT0bEsC1EUqVarAXQ8tslX6hTLFaR2+7+UEDTxQWp8cpIqsLyRZCWaZfuygrAa4zxTRmk10BWJs/siRzdFlOY3PVUiEr1jN55H/npjMg1bC/N3XB+ckcFe4gFhPc5WskbmvcPSZoL9qwJmV8Of2hzfltiJ5Rh2ZHB+iBymWTtN05ebODM3BLqui/u30bNikPvQ8dwwrLYHvMh98MO81R9Rb3fBC/NXbUiloYM3w//b8BdvaWem74XsPgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"normalization variants\"\n        title=\"\"\n        src=\"/static/824c12817bef26d5c045bf9425d0533d/c1b63/normalization-variants.png\"\n        srcset=\"/static/824c12817bef26d5c045bf9425d0533d/5a46d/normalization-variants.png 300w,\n/static/824c12817bef26d5c045bf9425d0533d/0a47e/normalization-variants.png 600w,\n/static/824c12817bef26d5c045bf9425d0533d/c1b63/normalization-variants.png 1200w,\n/static/824c12817bef26d5c045bf9425d0533d/d61c2/normalization-variants.png 1800w,\n/static/824c12817bef26d5c045bf9425d0533d/1d7f7/normalization-variants.png 2038w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>Image from <a href=\"https://arxiv.org/abs/1803.08494\">Group Normalization, 2018</a></p>\n<p>이후에 나온 위 연구에서는 Batch 단위로 정규화를 진행하는 것에서 나아가, Layer 단위 정규화, 하나의 데이터 샘플에서의 정규화, 몇개의 데이터 샘플에서의 정규화 방식의 장단점에 대해 다루었다.</p>\n<br>\n<br>\n<h2>Reference</h2>\n<ul>\n<li>boostcourse, 딥러닝 기초 다지기</li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"[DL] Regularization","date":"June 09, 2023"}}},"pageContext":{"slug":"/AI/DL/regularization/","previous":{"fields":{"slug":"/AI/DL/optimization-2/"},"frontmatter":{"title":"[DL] 최적화 방식"}},"next":null}},"staticQueryHashes":["1185972000","3231742164"],"slicesMap":{}}