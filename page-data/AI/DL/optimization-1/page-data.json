{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/DL/optimization-1/","result":{"data":{"allMarkdownRemark":{"totalCount":51},"markdownRemark":{"id":"38af05ae-3dfa-5d3e-9391-b8d5c21a030f","html":"<h2>1. 최적화</h2>\n<p>최적화는 컴퓨터 공학의 중요한 주제이다. 최적화가 필요한 문제는 대체로 문제 해결을 위한 사전 정보가 부족하거나 일괄적인 법칙으로 해결할 수가 없다. 다시말해 최적화는 탐색과 문제 해결을 동시에 진행할 수 있는 방법이며, 사전 정보가 크게 필요하지 않다. 최적화는 크게 조합 최적화와 연속적 최적화로 나눌 수 있는데, 여기서는 신경망이 활용하는 Gradient Descent라는 연속적 최적화 방식에 대해 다루고자 한다.</p>\n<br>\n<h3>1.1 Gradient Descent</h3>\n<p>Gradient Descent(GD)란 미분가능한 함수의 국소적 최소점(local minimum)을 찾는 재귀적인 최적화 알고리즘이다. 특히 딥러닝 또는 신경망의 맥락에서는 정의한 loss 함수를 최소화하기 위해 다변수 함수인 loss의 그래디언트를 활용해 weight와 loss를 재귀적으로 업데이트하는 알고리즘을 일컫는다. 그림으로 이해하면 다음과 같다:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC6UlEQVR42m3STVMSARjA8QeoD9Cp9FyHrh26dFDJMhunGkcdMystm0ZrJhobdaZxUMrCIlOzckxLx3zrZWRMyjLfIlIk3whKJDCRRZZll91FUthln2Y6dLHfB/if/nA0/UaDMkVNK1PUq8pk9cbhVI2Yk6nzlKk6HvV1GPeOGObAuxIERAQ6yMPS4hoM9poVAy8tB3raP9/qfvZpd3vzKLx4bpI/uPcGIDfnzs7M41rn2bxGvHD2Ubwg7wHmZdehqrgNm3QGcdgw/+GXO3AuGhP2BSle6XSsaWam3N9MY4s49t6G4x/stZOfnPC0eXR7YsJ5GRzLqIHsrNo9yUmVoaSkSjx0sCp+NO1mvDC/Sai42on3bw/g654vaDYtSY7vBC67AugjWKTpSJymI0j6eR8i7tBp9QBwCCBVWbVdmayG9CM3S9NSb+CJjFoxP6dBOpPbKF0qapGul3YJdzV6sa/DhLPTbokNRYT45m8ReUpExieK6xz6vLS9TqvvKCpouga31K9kuho96Gr6E8pVnWx1RR82aAel+tuDkraqHx/eeyf1907HpoxOcdUTksToprThtguseQTD8yZkf7n0bwdnistV7ckFpxt2wcSQDV52meX6Xgu87p7UDw3Mo2XSHVuY9aBxdDE+bLDinGUZSYLGWJiP8sSqwCz9QN6xMOG3ft2PkgMmxmxQpmqHU7l18Nd3q0e+MLMC1rmVJJJgMBrmpDhPi2yAwWUXGSG8TCPl9f9cd9oxZJtFxkt0tiDKyB82QKTk4yPWbVdKWuUlFx/DP6xtRrHhccFmgNDE5z5ibKQ7ylrGUWCp05yfBI5iEgXSUy3R3kLOPAYEInDLS4ruqSD8l+Sa/fsaAoBoaO3h76jQ9+SuyXDqMhjb+hVM6DeIQQKQIWDtoQYYmpWxFANBwr815lsNAuuwQ3hqWIa4AesnD0C45nI9d70oK6zKBr66RMGRpEygCEXM71EwJAVcgIII6dsS+wPKMesNAVGJpwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"http://primo.ai/index.php?title=Gradient_Descent_Optimization_%26_Challenges\"\n        title=\"\"\n        src=\"/static/9f01c58e3756531ab6b7f33723679af6/5a190/gradient-descent.png\"\n        srcset=\"/static/9f01c58e3756531ab6b7f33723679af6/5a46d/gradient-descent.png 300w,\n/static/9f01c58e3756531ab6b7f33723679af6/0a47e/gradient-descent.png 600w,\n/static/9f01c58e3756531ab6b7f33723679af6/5a190/gradient-descent.png 800w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nfrom <a href=\"http://primo.ai/index.php?title=Gradient_Descent_Optimization_%26_Challenges\">here</a></p>\n<p>예를 들어 사전에 정의한 loss 함수가 위와 같은 2차원 표면이라면, 랜덤한 포인트에서 시작해 화살표가 가리키는 가장 낮은 지점으로 이동하는 것이 알고리즘의 목적이다. 즉 최소점으로 도달하는 목적을 가지고 경로를 최대한 단축시키기 위해서 gradient가 가장 큰 (즉 가장 가파른) 지점으로 점프해나가는 전략이다. 일반적으로 GD 알고리즘은 다음과 같이 쓸 수 있다:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>W</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>W</mi><mi>n</mi></msub><mo>−</mo><mi>α</mi><mtext> </mtext><mi>d</mi><msub><mi>W</mi><mi>n</mi></msub><mspace linebreak=\"newline\"></mspace><msub><mi>b</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>b</mi><mi>n</mi></msub><mo>−</mo><mi>α</mi><mtext> </mtext><mi>d</mi><msub><mi>b</mi><mi>n</mi></msub><mspace linebreak=\"newline\"></mspace></mrow><annotation encoding=\"application/x-tex\">W_{n+1} = W_{n} - \\alpha \\, d W_{n} \\\\\nb_{n+1} = b_{n} - \\alpha \\, d b_{n} \\\\</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8917em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9028em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mspace newline\"></span></span></span></span></div>\n<p>이 식은 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">d W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>와 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">d b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">b</span></span></span></span></span>가 각각 변수 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span>에 대한 loss의 그래디언트일 때 gradient descent 방식을 나타낸다. 위 식을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">n = 1, ...</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8389em;vertical-align:-0.1944em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">...</span></span></span></span></span>에 대해 시행하면 재귀적 알고리즘이 된다. 이때 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span>는 최소점에 달하기 위한 보폭으로, 모델 또는 알고리즘의 하이퍼 파라미터이며 일반적으로 learning rate(학습률)이라고 부른다.</p>\n<br>\n<br>\n<h2>2. 최적화 방법</h2>\n<h3>2.1 Stochastic / Mini-batch Gradient Descent</h3>\n<p>고정된 크기의 데이터 샘플을 하나의 행렬로 만드는 것을 벡터화(Vectorization) 또는 배치(Batch)로 묶는다고 하며, 이를 통해 계산 효율을 얻을 수 있다. GD 또한 배치의 크기에 따라 종류가 나뉜다.</p>\n<ul>\n<li>Stochastic Gradient Descent (SGD)</li>\n<li>Mini-batch Gradient Descent</li>\n<li>Batch Gradient Descent (BGD)</li>\n</ul>\n<p>Stochastic GD는 하나의 데이터 샘플에 대해 GD를 진행하는 것이고, Mini-batch GD는 1보다는 크고 전체 데이터 샘플 수보다는 작은 batch 사이즈에 대해, Batch GD는 전체 데이터 샘플 수와 배치 크기가 같은 경우의 GD를 일컫는다. 따라서 SGD는 속도가 빠르고 정확성이 떨어지는 반면 BGD는 속도가 느리고 정확성이 높아서, 특히 데이터셋의 규모가 큰 경우 BGD 보다는 Mini-batch GD 방식을 활용했을 때 학습속도가 더 빠르다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 908px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/webp;base64,UklGRkgAAABXRUJQVlA4IDwAAABwAwCdASoUAAoAPtFWo0uoJKMhsAgBABoJaQAAW+zHb/u9mkAA/vYSct2teieZwOIk8Ev66eIvqfWAAAA='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461\"\n        title=\"\"\n        src=\"/static/a7db4cafcf8196d7ae11f8c83c6196a8/1f8a9/stochastic-batch-gd.webp\"\n        srcset=\"/static/a7db4cafcf8196d7ae11f8c83c6196a8/c85cb/stochastic-batch-gd.webp 300w,\n/static/a7db4cafcf8196d7ae11f8c83c6196a8/e88ff/stochastic-batch-gd.webp 600w,\n/static/a7db4cafcf8196d7ae11f8c83c6196a8/1f8a9/stochastic-batch-gd.webp 908w\"\n        sizes=\"(max-width: 908px) 100vw, 908px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nfrom <a href=\"https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461\">here</a></p>\n<p>예를 들어 학습 데이터셋의 크기가 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>50</mn><mo separator=\"true\">,</mo><mn>000</mn><mo separator=\"true\">,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">n=50,000,000</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8389em;vertical-align:-0.1944em;\"></span><span class=\"mord\">50</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">000</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">000</span></span></span></span></span> 이고 미니 배치 크기가 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">m=1,000</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8389em;vertical-align:-0.1944em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">000</span></span></span></span></span> 이라면, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mo separator=\"true\">,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">5,000</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8389em;vertical-align:-0.1944em;\"></span><span class=\"mord\">5</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">000</span></span></span></span></span>번 GD를 수행하게 된다.\npsuedo-code는 다음과 같다.</p>\n<pre class=\"grvsc-container abyss\" data-language=\"\" data-index=\"0\"><code class=\"grvsc-code\"><span class=\"grvsc-line\"><span class=\"grvsc-source\">for t=1, ..., 5000 {</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">    Forward prop on X_t</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        Z_i = W_i X_t + b_i</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        A_i = g_i * Z_i</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        ...</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        A_l = g_l * Z_(l-1)</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">    Compute cost gradients w.r.t. J_t</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        w_l = W_l - alpha * dW_l</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        b_l = b_l - alpha * db_l</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">}</span></span></code></pre>\n<br>\n<h3>2.2 Learning Rate Decay</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABHUlEQVR42l1RXW+DMAzk//+zPU6jVctLNQGig3YUgfNNbrqgVDCkk52z7+KYAgBijMhxXVd47+GcgzHmAHLe+dSz1+zzIh9oYq09GCilNmhGfagzp2ZvmgxJ8GY2bBM4rMHDWYsQPIJzCHabWBuNfvnFpOc0KTlib1pwfJrx4JyFGIdhnDGJQd1P+Kp/8PF9w+15x1NNeCwvGGeTYQghGTIenkxDIEK0wbgYPKYFs7YYRsFn3aG8N5iVRuR+nUdcY3pu3jdj/goS3JOIoHu80A4jrNHQSqC1giwCI/rdI0reeY75JxJF3/cYhgFd16FtWzRN80Zd14kjmO9r5Kihlh4ZRVVV2ONyueB8PuN0OqEsywPIscae6/WK/1riD0LnZRIn0iXeAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"DeepLearning.AI\"\n        title=\"\"\n        src=\"/static/f0fbdbbe7b4b69551c8d559730aa1ff2/c1b63/learning-rate-decay.png\"\n        srcset=\"/static/f0fbdbbe7b4b69551c8d559730aa1ff2/5a46d/learning-rate-decay.png 300w,\n/static/f0fbdbbe7b4b69551c8d559730aa1ff2/0a47e/learning-rate-decay.png 600w,\n/static/f0fbdbbe7b4b69551c8d559730aa1ff2/c1b63/learning-rate-decay.png 1200w,\n/static/f0fbdbbe7b4b69551c8d559730aa1ff2/d61c2/learning-rate-decay.png 1800w,\n/static/f0fbdbbe7b4b69551c8d559730aa1ff2/97a96/learning-rate-decay.png 2400w,\n/static/f0fbdbbe7b4b69551c8d559730aa1ff2/b325c/learning-rate-decay.png 2770w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nfrom DeepLearning.AI</p>\n<p>학습률 감소는 학습이 진행됨에 따라 학습률을 감소시키는 전략이다. 앞서 학습의 정확성과 속도가 일반적으로 trade-off 관계에 있다는 것을 알았다. 따라서 Learning rate decay 전략으로 학습 초기에는 큰 학습률을 통해 빠른 속도로 최저점 주변으로 이동한 후 학습률을 낮추면, 최저점에 수렴하지 않을 가능성이 낮아지며(정확도 향상) 전체 학습 시간은 처음부터 작은 학습률을 활용한 것보다 빨라지는 효과를 얻을 수 있다.</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>α</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mtext> </mtext><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><mo>∗</mo><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext> </mtext><mi>n</mi><mi>u</mi><mi>m</mi></mrow></mrow></mfrac><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha = \\frac{1}{1 + \\mathit{decay \\,rate} * \\mathit{epoch \\,num}} \\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2019em;vertical-align:-0.8804em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3214em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathit\">decay</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathit\">rate</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathit\">epoch</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathit\">num</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></div>\n<p>위와 같은 방식으로 Learning Rate Decay를 적용하면 <code>decay_rate</code> 또한 모델의 하이퍼 파라미터로 작용한다. 만약 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha_0 = 0.2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0.2</span></span></span></span></span>이고 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mtext> </mtext><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\mathit{decay \\,rate}= 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathit\">decay</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathit\">rate</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> 이라면 epoch=1에서는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0.1</span></span></span></span></span>, epoch=2에서는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.67</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.67</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0.67</span></span></span></span></span>, epoch=3에서는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0.5</span></span></span></span></span> 임을 알 수 있다. 또는 manual 방식으로 Learning Rate Decay를 정의할 수도 있다:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.9</mn><msup><mn>5</mn><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext> </mtext><mi>n</mi><mi>u</mi><mi>m</mi></mrow></msup><mo>∗</mo><msub><mi>α</mi><mn>0</mn></msub><mspace linebreak=\"newline\"></mspace><mi>α</mi><mo>=</mo><mfrac><mi>k</mi><msqrt><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext> </mtext><mi>n</mi><mi>u</mi><mi>m</mi></mrow></msqrt></mfrac><mo>∗</mo><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha = 0.95^{\\mathit{epoch \\,num}} * \\alpha_0 \\\\\n\\alpha = \\frac{k}{\\sqrt{\\mathit{epoch \\,num}}} * \\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8991em;\"></span><span class=\"mord\">0.9</span><span class=\"mord\"><span class=\"mord\">5</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathit mtight\">epoch</span><span class=\"mspace mtight\" style=\"margin-right:0.1952em;\"></span><span class=\"mord mathit mtight\">num</span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.3014em;vertical-align:-0.93em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.275em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.835em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathit\">epoch</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathit\">num</span></span></span></span><span style=\"top:-2.795em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\"><path d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\"></path></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.205em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></div>\n<br>\n<h3>2.3 모멘텀이 있는 Gradient Descent</h3>\n<p>Gradient descent에 모멘텀을 적용하면 대부분의 경우에 더 <strong>빠른 속도</strong>로 작동한다. 일반적으로 Gradient Descent는 수직 방향으로 진동하는 형태로 최소점에 수렴하는데, 이런 진동(oscillation)은 최적화 속도를 감소시키며 큰 learning rate를 사용할 때 발산하는 잠재적 문제를 가지고 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 16.333333333333332%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAjklEQVR42mWOywrCMBBF+/+fp26KhoIabebVYPrKplcSaEFcHJjF3MNptm1DIecMjR+8WUEslcCCPhBe3leYGWZ2EGPEOI51u3saAJjnGSklXJzHqetxLjiPtnui7e7orzcE5/DwHsIMVa3/RSYi9V6WpajQ7OZ1zZCYYDZAVCuDGYjCUUhEf4XTNP0UfgH1k+PGxkVFoQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"DeepLearning.AI\"\n        title=\"\"\n        src=\"/static/c5304823773c81c6541544e6c78bae9f/c1b63/gradient-descent-oscillation.png\"\n        srcset=\"/static/c5304823773c81c6541544e6c78bae9f/5a46d/gradient-descent-oscillation.png 300w,\n/static/c5304823773c81c6541544e6c78bae9f/0a47e/gradient-descent-oscillation.png 600w,\n/static/c5304823773c81c6541544e6c78bae9f/c1b63/gradient-descent-oscillation.png 1200w,\n/static/c5304823773c81c6541544e6c78bae9f/d61c2/gradient-descent-oscillation.png 1800w,\n/static/c5304823773c81c6541544e6c78bae9f/97a96/gradient-descent-oscillation.png 2400w,\n/static/c5304823773c81c6541544e6c78bae9f/97755/gradient-descent-oscillation.png 2536w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nfrom DeepLearning.AI</p>\n<p>위에서 빨간 점이 최소점이라고 하면, 우리가 원하는 것은 위아래로 더 적게 움직이지만 좌우로는 더 빨리 수렴하는 알고리즘이다. 즉, 마치 물체가 운동에 대한 모멘텀을 가지는 것 같은 효과를 반영하는 것이 목적이다. 모멘텀이 있는 Gradient descent를 계산하는 방법은 <strong>지수가중 평균(exponential weighted average)</strong> 을 이용하는 것이다. 우선 미니 배치에 대해 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">dW</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span> 와 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">db</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">b</span></span></span></span></span> 를 계산하고, 다음과 같이 smoothing을 위한 지수가중 평균을 계산한다.</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>v</mi><mrow><mi>d</mi><mi>W</mi></mrow></msub><mo>:</mo><mo>=</mo><mi>β</mi><msub><mi>v</mi><mrow><mi>d</mi><mi>W</mi></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>W</mi><mspace linebreak=\"newline\"></mspace><msub><mi>v</mi><mrow><mi>d</mi><mi>b</mi></mrow></msub><mo>:</mo><mo>=</mo><mi>β</mi><msub><mi>v</mi><mrow><mi>d</mi><mi>b</mi></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>b</mi><mspace linebreak=\"newline\"></mspace></mrow><annotation encoding=\"application/x-tex\">v_{dW} := \\beta v_{dW} + (1- \\beta) dW \\\\\nv_{db} := \\beta v_{db} + (1- \\beta) db \\\\</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">b</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">b</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">b</span></span><span class=\"mspace newline\"></span></span></span></span></div>\n<p>이 연산으로 인해 수직 방향의 평균은 0에 가까운 값이 되고, 수평 방향의 평균은 여전히 큰 값으로 남아서 빠른 최적화를 수행할 수 있다. 이때 각 변수의 도함수 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">dW</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>와 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">db</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">b</span></span></span></span></span>는 가속 효과를 나타내고, 모멘텀 항 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>V</mi><mrow><mi>d</mi><mi>W</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">V_{dW}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>V</mi><mrow><mi>d</mi><mi>b</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">V_{db}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mord mathnormal mtight\">b</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>는 속도를 나타낸다. <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span>는 0보다 작은 속도의 계수 값(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>≃</mo></mrow><annotation encoding=\"application/x-tex\">\\simeq</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4637em;\"></span><span class=\"mrel\">≃</span></span></span></span></span> 0.9)으로, 마찰력을 나타낸다.</p>\n<pre class=\"grvsc-container abyss\" data-language=\"\" data-index=\"1\"><code class=\"grvsc-code\"><span class=\"grvsc-line\"><span class=\"grvsc-source\">for t=1, ..., 5000 {</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">    Forward prop on X_t</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        Z_i = W_i X_t + b_i</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        A_i = g_i * Z_i</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        ...</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        A_l = g_l * Z_(l-1)</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">    Compute cost gradients w.r.t. J_t</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        v_dW = beta * v_dW + (1-beta) * dW</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        v_db = beta * v_db + (1-beta) * db</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        w_l = W_l - alpha * v_dW</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">        b_l = b_l - alpha * v_dbs</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">}</span></span></code></pre>\n<br>\n<br>\n<h2>Reference</h2>\n<ul>\n<li>DeepLearning.AI, Deep Learning Specialization</li>\n<li><a href=\"https://cs231n.github.io/neural-networks-3/\">CS231N</a></li>\n<li><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\">Medium, Stochastic Gradient Descent with momentum</a></li>\n<li>boostcourse, 딥러닝 기초 다지기</li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n  .abyss { background-color: #000c18; }\n  .abyss .grvsc-line-highlighted::before {\n    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));\n    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));\n  }\n</style>","frontmatter":{"title":"[DL] Gradient Descent 최적화","date":"June 05, 2023"}}},"pageContext":{"slug":"/AI/DL/optimization-1/","previous":{"fields":{"slug":"/CS/Web/utterances/"},"frontmatter":{"title":"[Gatsby] utterances 추가하기"}},"next":{"fields":{"slug":"/AI/DL/vscode-setting/"},"frontmatter":{"title":"[DL] Apple M1에서 VS code 환경 세팅하기"}}}},"staticQueryHashes":["1185972000","3231742164"],"slicesMap":{}}