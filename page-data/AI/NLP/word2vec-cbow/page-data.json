{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/NLP/word2vec-cbow/","result":{"data":{"allMarkdownRemark":{"totalCount":12},"markdownRemark":{"id":"a78d5036-beb0-5e6f-918c-05187421f263","html":"<p><code>word2vec</code>은 2013년 구글에서 고안한 자연어 처리 아이디어로, 이에 기반한 모델은 <code>Continuous Bag-of-Words(CBOW)</code>와 <code>Skip-gram</code> 두가지가 있다. 이 글은 그 중에서 <code>CBOW</code> 모델을 원 논문과 deeplearning.ai 수업을 참고하여 정리한 글이다.</p>\n<p><strong>원 논문</strong>:</p>\n<ul>\n<li>Mikolov et. al., 2013, Efficient Estimation of Word Representations in Vector Space (<a href=\"https://arxiv.org/pdf/1301.3781.pdf\">arxiv</a>)</li>\n<li>Mikolov et. al., 2013, Distributed Representations of Words and Phrases and their Compositionality (<a href=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\">arxiv</a>)</li>\n</ul>\n<br>\n<br>\n<h2>0. CBOW란</h2>\n<p>CBOW(Continuous Bag-of-Words, 연속되는 단어 주머니)는</p>\n<ul>\n<li>텍스트 데이터를 벡터 공간에 표현하는 <code>단어 임베딩(word embedding)</code> 모델이자,</li>\n<li><code>얕은 신경망(neural network)</code> 모델이며,</li>\n<li>스스로 훈련 데이터를 생성하는 <code>자기 지도 훈련(self-supervised learning)</code> 모델이다.</li>\n</ul>\n<p>CBOW 모델이 처음 소개될 때는 50-100 차원의 원 핫 벡터로 몇 백만개의 단어를 훈련시켰다.</p>\n<h3>📖 Skip-gram과의 차이점</h3>\n<p>CBOW 모델은 여러개의 단어 데이터를 입력하면 그에 상응하는 한개의 단어를 출력하는 <code>Many to One</code> (여러개 데이터를 입력받아 한개의 데이터를 출력하는 모델 구조) 모델이다. 반면 Skip-gram은 한개의 단어를 입력했을 때 그에 대응하는 여러개의 단어를 출력하는 <code>One to Many</code> 모델이다. 즉 두 모델의 구조는 <code>반전</code>되어있고, 입력값과 출력값이 서로 반대된다.</p>\n<br>\n<br>\n<h2>1. 모델의 구조</h2>\n<p>CBOW는 얕은 신경망 모델로, 이 글에서는 한 개의 은닉층(hidden layer)를 가지는 신경망 모델을 고려한다.</p>\n<!--<figure style=\"width: 500px\" class=\"align-center\">-->\n<!--    <a href=\"/imgs/post-imgs/ml-cbow-model_architecture.png\"><img src=\"/imgs/post-imgs/ml-cbow-model_architecture.png\"></a>-->\n<!--    <figcaption>image by DeepLearning.AI</figcaption>-->\n<!--</figure>-->\n<p>모델의 흐름은 다음과 같다.</p>\n<ol>\n<li>텍스트 데이터를 원 핫 벡터로 변환한다.</li>\n<li>첫번째 은닉층(hidden layer)을 거친다.\n<ul>\n<li>활성화 함수 : ReLU</li>\n</ul>\n</li>\n<li>두번째 결과층(output layer)을 거친다.\n<ul>\n<li>활성화 함수 : Softmax</li>\n</ul>\n</li>\n<li>결과 벡터의 값 중 가장 큰 값으로 예측한다.</li>\n</ol>\n<p>모델을 이해하고 실제로 구현하기 위해서는 각 층의 차원을 정확히 알아야 한다.</p>\n<br>\n<h3>📖 벡터의 차원</h3>\n<p>변수를 다음과 같이 정의할 때,</p>\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span> : 단어 사전의 크기,  혹은 원-핫 벡터의 크기.</li>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> : 임베딩 크기. 모델의 하이퍼파라미터이다.</li>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span> : 배치 크기. 한번에 훈련할 데이터의 개수이다.</li>\n</ul>\n<p>입력값 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span>의 차원</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>X</mi><mo>∈</mo><mi>M</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">X \\in M(V, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p>에 대해 각 층에 대한 벡터의 차원을 다음과 같이 정리할 수 있다.</p>\n<center>\n|은닉층 벡터|차원|결과층 벡터|차원|\n| --- | --- | --- | --- |\n|$W_1$|$(N, V)$|$W_2$|$(V, N)$|\n|$B_1$|$(N, m)$|$B_2$|$(V, m)$|\n|$z_1$|$(N, m)$|$z_2$|$(V, m)$|\n|$relu(z_1)$|$(N, m)$|$softmax(z_2)$|$(V, m)$|\n</center>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo>≡</mo><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">softmax(z_2) \\equiv \\hat{Y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">so</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">x</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.044em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≡</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9468em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span></span></span></span></span></span></span></span> 이므로 예측값이 입력값과 같은 차원을 가지는 것을 알 수 있다. 즉 모델이 반환하는 벡터의 열 벡터는 입력 열 벡터와 순서가 같은 원 핫 벡터이다.</p>\n<br>\n<br>\n<h2>2. 모델의 전처리</h2>\n<p>CBOW 모델로 문장의 빈칸을 주위 단어에 기반해 예측하는 과제를 수행해보자. 다음 문장의 빈칸에 뭐가 들어갈까?</p>\n<p>\"npm은 Node.js의 ____ 관리를 위한 패키지 매니저이다.\"</p>\n<p>CBOW 모델을 구현하기 위해서는 텍스트 데이터를 토큰화 한 후, 데이터를 모델에 입력하는 형태로 변환하는 다음 작업이 필요하다.</p>\n<br>\n<h3>📖  중심어(center word)와 맥락 단어들(context words)</h3>\n<p>자기 지도 학습은 사람이 라벨링을 할 필요가 없다는 장점이 있다. 그러기 위해서는 가공되지 않은 텍스트 데이터에서 훈련 데이터(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span>, 입력 데이터)와 훈련 타겟(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span>, 참값)을 구분해서 자료화할 필요가 있다.</p>\n<p>CBOW 모델에서 예측할 대상(target)인 문장의 빈칸을 <code>중심어</code>로, 이 단어와 문장 내에서 인접한 단어를 <code>맥락 단어</code>로 이름지을 수 있다. <code>맥락 단어</code>는 중심어로 부터 거리 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span> 만큼 떨어져 있는 인접한 단어들로 정의하며, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span>를 <code>맥락의 절반 크기(context half-size)</code>라고 하자. <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span>는 모델의 성능을 좌우하는 하이퍼파라미터 중 하나이다. 왼쪽 맥락 단어 리스트와 중심어, 오른쪽 맥락 리스트의 크기를 모두 더한 값을 <code>윈도우</code>라고 일컫는다.</p>\n<p>예를 들면, 문장 한개로 구성된 데이터에 대해 다음과 같이 이해할 수 있다.</p>\n<pre class=\"grvsc-container abyss\" data-language=\"python\" data-index=\"0\"><code class=\"grvsc-code\"><span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk3\"># given tokenized data and context half-size, </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk3\"># returns center word and list of context words </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk15 mtki\">def</span><span class=\"mtk1\"> </span><span class=\"mtk6\">center_and_context_word</span><span class=\"mtk1\">(</span><span class=\"mtk19 mtki\">data</span><span class=\"mtk1\">, </span><span class=\"mtk19 mtki\">C</span><span class=\"mtk1\">):</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk7\">for</span><span class=\"mtk1\"> i </span><span class=\"mtk7\">in</span><span class=\"mtk1\"> </span><span class=\"mtk15\">range</span><span class=\"mtk1\">(C, </span><span class=\"mtk15\">len</span><span class=\"mtk1\">(data)</span><span class=\"mtk7\">-</span><span class=\"mtk1\">C):</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">        center_word </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> data[i]</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">        context_words </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> []</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">        </span><span class=\"mtk7\">for</span><span class=\"mtk1\"> j </span><span class=\"mtk7\">in</span><span class=\"mtk1\"> </span><span class=\"mtk15\">range</span><span class=\"mtk1\">(i</span><span class=\"mtk7\">-</span><span class=\"mtk1\">C, i</span><span class=\"mtk7\">+</span><span class=\"mtk1\">C</span><span class=\"mtk7\">+</span><span class=\"mtk4\">1</span><span class=\"mtk1\">):</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">            </span><span class=\"mtk7\">if</span><span class=\"mtk1\"> j </span><span class=\"mtk7\">!=</span><span class=\"mtk1\"> i:</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">                context_words.append(data[j])</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">        </span><span class=\"mtk7\">yield</span><span class=\"mtk1\"> center_word, context_words</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">C </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> </span><span class=\"mtk4\">2</span><span class=\"mtk1\"> </span><span class=\"mtk3\"># context half-size</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">data </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> [</span><span class=\"mtk11\">&quot;npm은&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;Node.js의&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;패키지&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;관리를&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;위한&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;패키지&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;매니저이다&quot;</span><span class=\"mtk1\">, </span><span class=\"mtk11\">&quot;.&quot;</span><span class=\"mtk1\">]</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">center_word, context_word </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> </span><span class=\"mtk15\">next</span><span class=\"mtk1\">(center_and_context_word(data, C))</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk15\">print</span><span class=\"mtk1\">(center_word) </span><span class=\"mtk3\"># &quot;패키지&quot;</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk15\">print</span><span class=\"mtk1\">(context_word) </span><span class=\"mtk3\"># [&quot;npm은&quot;, &quot;Node.js의&quot;, &quot;관리를&quot;, &quot;위한&quot;]</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk15\">print</span><span class=\"mtk1\">(</span><span class=\"mtk15\">len</span><span class=\"mtk1\">(context_word </span><span class=\"mtk7\">+</span><span class=\"mtk1\"> center_word)) </span><span class=\"mtk3\"># 5, window</span></span></span></code></pre>\n<p>모델의 입력값은 <code>맥락 단어 벡터들의 평균값</code>을 취한다. 사실 CBOW 모델 이름에 Bag이 들어가는 이유는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span>의 범위 내에 있는 맥락 단어들이 문장에서의 순서에 관계없이 여겨지기 때문이고, 이 특징은 이후에 등장하는 Sequential 모델과 구분되는 차이점이다.</p>\n<br>\n<br>\n<h2>3. 모델 훈련하기</h2>\n<p>CBOW 모델은 신경망 모델이므로 일반적인 forward propagation, backward propagation, gradient descent 과정을 거친다. 세가지 과정을 <code>keras</code> 라이브러리에서 <code>Layer</code> 객체로 비교적 간단하게 구현할 수 있다.</p>\n<br>\n<h3>📂 Keras로 CBOW 구현하기</h3>\n<pre class=\"grvsc-container abyss\" data-language=\"python\" data-index=\"1\"><code class=\"grvsc-code\"><span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk7\">from</span><span class=\"mtk1\"> tf.keras </span><span class=\"mtk7\">import</span><span class=\"mtk1\"> layers</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk3\"># Input size: (batch_size, vocab_size)</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">cbow_model </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> tf.keras.Sequential{</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk3\"># 원 핫 벡터의 배치를 임베드한다</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    input_layer = layers.Embedding(</span><span class=\"mtk19 mtki\">input_dim</span><span class=\"mtk7\">=</span><span class=\"mtk1\">vocab_size, </span><span class=\"mtk19 mtki\">output_dim</span><span class=\"mtk7\">=</span><span class=\"mtk1\">embed_dim), </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk3\"># relu 은닉층으로 비용이 음수값을 가지지 않게 한다</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    hidden_layer = layers.Dense(</span><span class=\"mtk19 mtki\">units</span><span class=\"mtk7\">=</span><span class=\"mtk1\">embed_dim,</span><span class=\"mtk19 mtki\">activation</span><span class=\"mtk7\">=</span><span class=\"mtk11\">&#39;relu&#39;</span><span class=\"mtk1\">), </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk3\"># 원 핫 벡터의 배치를 확률로 출력한다</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    output_layer = layers.Dense(</span><span class=\"mtk19 mtki\">input_dim</span><span class=\"mtk7\">=</span><span class=\"mtk1\">vocab_size, </span><span class=\"mtk19 mtki\">activation</span><span class=\"mtk7\">=</span><span class=\"mtk11\">&#39;softmax&#39;</span><span class=\"mtk1\">)</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">}</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">batch_size </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> </span><span class=\"mtk4\">256</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">epochs </span><span class=\"mtk7\">=</span><span class=\"mtk1\"> </span><span class=\"mtk4\">10</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">cbow_model.compile(</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk19 mtki\">optimizer</span><span class=\"mtk7\">=</span><span class=\"mtk11\">&#39;Adam&#39;</span><span class=\"mtk1\">, </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk19 mtki\">loss</span><span class=\"mtk7\">=</span><span class=\"mtk1\">tf.keras.losses.CategoricalCrossentropy()</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">)</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">cbow_model.fit(</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    train_data, train_target, </span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">    </span><span class=\"mtk19 mtki\">batch_size</span><span class=\"mtk7\">=</span><span class=\"mtk1\">batch_size, </span><span class=\"mtk19 mtki\">epochs</span><span class=\"mtk7\">=</span><span class=\"mtk1\">epochs</span></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"><span class=\"mtk1\">)</span></span></span></code></pre>\n<br>\n<br>\n<h2>4. 단어 임베딩 추출하기</h2>\n<p>단어 임베딩은 원 핫 벡터에 비해 <code>밀도가 높은(Dense) 벡터</code>로, 단어 임베딩에는 여러가지 이점이 있다. 첫째로 단어 임베딩 벡터간의 거리를 비교해서 의미론적(semantic)이고 문법론적인(syntactic) 정보를 얻을 수 있다. 둘째로 차원을 작게 만드는 것, 즉 <code>차원 축소(Dimensionality Reduction)</code>를 통해 계산 횟수를 획기적으로 줄일 수 있다. 벡터의 밀도가 높다는 것은 같은 데이터를 상대적으로 작은 차원으로 표현하는 것을 뜻한다. 반면에 벡터의 차원이 증가함에 따라 벡터를 계산하는 횟수는 기하급수적으로 늘어나게 되는데, 이 현상을 <code>차원의 저주(the curse of dimensionality)</code>라고 한다. 그 중에서 2차원이나 3차원 벡터는 시각화가 가능하므로 직관적인 이해에 도움이 된다.</p>\n<p>단어 임베딩은 CBOW 모델의 부산물이라고 할 수 있는데, 단어 임베딩은 훈련이 끝난 후 그 결과인 가중치 벡터로 부터 얻을 수 있다.</p>\n<p>단어 임베딩으로 선택할 수 있는 <code>옵션</code>은 다음과 같다.</p>\n<ul>\n<li>첫번째 가중치 벡터 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> 의 열(column) 벡터</li>\n<li>두번째 가중치 벡터 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> 의 행(row) 벡터</li>\n<li>두 가중치 벡터의 평균 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo>∗</mo><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>W</mi><mn>2</mn><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">1/2 *(W_1 + W_2^{T})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1/2</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-2.4519em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2481em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span> 의 열 벡터</li>\n</ul>\n<p>마지막 경우는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo>∗</mo><mo stretchy=\"false\">(</mo><msubsup><mi>W</mi><mn>1</mn><mi>T</mi></msubsup><mo>+</mo><msub><mi>W</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">1/2 * (W_1^{T} + W_2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1/2</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-2.4519em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2481em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>의 행 벡터와 같다. 위의 모든 경우에 대해 <code>한개의 단어 임베딩 벡터의 크기</code>는 임베딩 크기 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span>에 대해 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N,1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span></span> 또는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1, N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span></span>인 것을 알 수 있다.</p>\n<br>\n<br>\n<h2>5. 모델 평가하기</h2>\n<p>모델을 평가하는 방법에는 크게 두가지가 있다.</p>\n<br>\n<h3>📖  내재적 평가와 외재적 평가</h3>\n<p>내재적 평가(Intrinsic Evaluation)는 임베딩된 단어들의 의미론적이고 문법론적인 관계를 평가하는 방법이다. 유의어(Analogies) 평가나 클러스터링 알고리즘, 또는 PCA 같은 시각화 기법들이 내재적 평가에 포함된다. 반면 외재적 평가(Extrinsic Evaluation)는 모델의 전체적인 성능을 파악하는데에 사용되는 방법이다. 전체 모델을 평가할 수 있지만, 평가 시간이 오래 걸리며 개선 방법에 대한 직관을 얻기 어렵다는 단점이 있다.</p>\n<br>\n<h3>📂  테스트 셋에 대해 (내재적으로) 평가하기</h3>\n<!--<figure style=\"width: 500px\" class=\"align-center\">-->\n<!--    <a href=\"/imgs/post-imgs/ml-cbow-test_set.png\"><img src=\"/imgs/post-imgs/ml-cbow-test_set.png\"></a>-->\n<!--    <figcaption>Table from Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space.</figcaption>-->\n<!--</figure>-->\n<p>위의 표는 4개의 모델을 두가지 훈련 데이터에 대해 평가한 결과이다. 첫번째 훈련 데이터는 <code>의미론적(semantic)</code>이고 <code>문법론적(syntactic)</code> 관계 정확도인데, CBOW가 의미론적 정확도는 Skip-gram보다 두배 이상 떨어지지만 문법적 정확도에서는 조금 더 나은 것을 알 수 있다. 그렇지만 의미론적 정확도에 비해 문법론적 정확도에서 평가 모델들의 편차가 더 적었다. 두번째 데이터 셋에 대해서는 CBOW가 Skip-gram보다 단어 관계 평가가 조금 더 나은 것을 볼 수 있다.</p>\n<p><em>참고 : 논문에서는 1억개가 넘어가지 않는 vocab에 대해 CBOW 모델을 훈련했으며, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">C=4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span></span></span></span></span> 설정에서 log-linear 분류로 최적의 결과를 얻었다고 한다.</em></p>\n<br>\n<br>\n<h2>출처</h2>\n<ol>\n<li>Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space</li>\n<li>Coursera, deeplearning.ai, NLP Specialization, Course 2, Natural Language Processing with Probabilistic Models</li>\n</ol>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n  .abyss { background-color: #000c18; }\n  .abyss .mtki { font-style: italic; }\n  .abyss .mtk3 { color: #384887; }\n  .abyss .mtk15 { color: #9966B8; }\n  .abyss .mtk1 { color: #6688CC; }\n  .abyss .mtk6 { color: #DDBB88; }\n  .abyss .mtk19 { color: #2277FF; }\n  .abyss .mtk7 { color: #225588; }\n  .abyss .mtk4 { color: #F280D0; }\n  .abyss .mtk11 { color: #22AA44; }\n  .abyss .grvsc-line-highlighted::before {\n    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));\n    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));\n  }\n</style>","frontmatter":{"title":"Word2Vec - CBOW","date":"March 31, 2022"}}},"pageContext":{"slug":"/AI/NLP/word2vec-cbow/","previous":null,"next":{"fields":{"slug":"/AI/NLP/naive-bayes/"},"frontmatter":{"title":"텍스트 나이브 베이즈 분류"}}}},"staticQueryHashes":["1185972000","3231742164"],"slicesMap":{}}