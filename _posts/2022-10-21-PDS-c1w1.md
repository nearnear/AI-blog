---
title: "Practical DS - Course 1 Week 1"
categories:
    - AWS
tags:
    - aws
header: 
    - img:
      url:
---

## Intro

### 실용적 데이터 사이언스와 클라우드 환경

<figure>
	<a href="/imgs/post-imgs/data_science.png"><img src
    ="/imgs/post-imgs/data_science.png"></a>
	<figcaption>The field of Data Science.</figcaption>
</figure>

데이터 사이언스는 {AI, ML, DL}과 {도메인 지식, 수학, 통계학, 시각화, 컴퓨터 공학}을 이어주는 간 학문(interdisciplinary field)이다.

나아가 실용적인 데이터 사이언스(PDS)란 방대한 양의 데이터셋을 분석하는 작업이다. 큰 데이터셋은 웹 또는 모바일 앱, 공적 자료나 기업의 자료를 바탕으로 할 수 있으며, 기본적으로 정리되지 않고(messy) 오류를 포함하고 있으며(error-ridden) 양식이 제멋대로(poorly documented)일 수 있다. 이런 데이터셋을 분석하고, 정제하며, 관련 특성을 추출하여 최종적으로 데이터를 증류(distillation)하고 데이터로부터 인사이트를 도출하는 작업이 실용적인 데이터 사이언스이다.

로컬 서버에 비해 클라우드에서 실행하는 실용적인 데이터 사이언스의 특징은 **agility와 elasticity**(신속함과 탄력성)이다. 예를 들어 모델 학습의 성공 여부가 컴퓨터의 연산 능력에 의해 결정될 때, 클라우드에서는 몇 초만에 새로운 연산 장치(compute instance)로 전환하여 학습을 계속할 수 있다([agile]). 또한 작업하는 데이터에 맞춘 연산 인스턴스를 이용할 수 있다([elastic]). 이때 두가지 시나리오가 있다:
- scaling up: 자원이 더 크거나 다른 종류의 연산 인스턴스로 변경하는 방법
- scaling out: 여러 연산 인스턴스를 병렬 이용하는 분산 학습을 하는 방법

또한 클라우드 환경에서 제공되는 Data Science와 ML toolbox를 활용할 수 있다.

#### [정리] 클라우드 사용의 이점
1. 신속하고 탄력적인 연산 장치의 전환, 즉 scaling up 또는 scaling out이 가능하다.
2. 클라우드에서 제공하는 Data Science와 ML toolbox를 이용할 수 있다.

### AWS를 활용하는 ML Workflow 개략

<figure>
	<a href="/imgs/post-imgs/machine_learning_workflow.png"><img src="/imgs/post-imgs/machine_learning_workflow.png"></a>
	<figcaption>ML Workflow using AWS toolboxes.</figcaption>
</figure>

Week 1에서는 데이터를 탐색하고 편향을 측정한다.
- Amazon S3로 데이터를 등록(ingest) 및 저장(store)하고, Athena로 데이터에 SQL 쿼리를 실행한다.
- Glue를 통해 schema에 따른 데이터 목록을 작성한다.
- SageMaker의 Data Wrangler와 Clarify 툴을 통해 통계 편향을 게산한다. 

Week 2에서는 SageMaker의 툴을 이용하여 Feature Engineering을 진행한다. Week 3에는 AutoML을 통해 베이스라인 모델과 최적(best) 모델 후보군을 생성하고, 커스텀 학습과 튜닝을 진행한다. Week 4에는 다양한 deployment 옵션과 전략을 알아보고 자동화된 파이프라인에서 모델 개발을 조직하는 방법을 알아본다.

### Week 1에서 시험할 Use case와 데이터셋

다양한 소스에서 수집한 고객의 상품 피드백의 감정을 분류하는 NLP/NLU 과제를 수행한다. 긍정(1), 중립(0), 부정(-1) 피드백으로 분류하며, 이를 통해 시장 트렌드와 고객 행동 분석, 나아가 상품의 잠재적 이슈를 알아내고자 한다.


## Tool 설명

### Amazon S3: 클라우드 기반 Data Lake

<figure>
	<a href="/imgs/post-imgs/ingest_data_into_data_lakes.png"><img src="/imgs/post-imgs/ingest_data_into_data_lakes.png"></a>
	<figcaption>Ingest data into data lakes.</figcaption>
</figure>

데이터 레이크(Data Lake)란 데이터의 크기나 타입에 관계 없이 저장, 탐색, 공유가 가능한 중앙화된 저장소로 간주할 수 있다. 데이터 레이크에는 데이터를 변형 없이 raw format으로 등록할 수 있다.

데이터 레이크는 Amazon S3같은 object storage 위에 세워진다. Object storage 시스템은 위계적 구조로 저장하는 file storage 시스템과 달리 고유한 id만으로 정의되는 block 단위로 데이터를 저장하는 방식이다. Object storage에서 데이터는 객체(object)로 간주되며, 그 자체로 또는 데이터 객체들을 저장하는 컨테이너인 데이터 버킷(Data bucket)으로 관리된다.

### [AWS Data Wrangler](https://github.com/aws/aws-sdk-pandas): Pandas on S3

Data Wrangler는 AWS에서 개발한 open source 파이썬 라이브러리로, pandas의 DataFrame과 AWS의 데이터 서비스를 연결한다.
- 즉, Pandas를 AWS S3에서 이용할 수 있게 한다.
- Data Wrangler로 다수의 data lake, data warehouse, 또는 database에서 데이터를 가져오거나 저장할 수 있다.

```python
import awswrangler as wr
import pandas as pd

# wr을 이용해 s3에서 직접 데이터를 읽어들일 수 있다.
df = wr.s3.read_csv(path='s3://bucket/prefix')
```

### [AWS Glue Data Catalog](https://aws.amazon.com/ko/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc): metadata tables for S3

<!-- $$
\begin{aligned}
    \text{Glue Data Catalog} \\
    \text{f : }
    & \text{S3} & \longrightarrow \text{Tables} \\
    & \text{data bucket} & \mapsto \text{Glue table}
\end{aligned}
$$ -->

Glue Data Catalog란 S3의 데이터 또는 데이터 버킷을 메타데이터 테이블로 보내는 mapping으로 이해할 수 있다. 따라서 S3에 있는 데이터를 직접 이동시키지 않고 메타 데이터를 저장한다. Glue 의 Glue table은 data schema 같은 메타데이터만 저장한다. 또한 자동으로 설정되는 Glue Crawler를 이용해 특정 자동으로 schema를 추론하거나 data catalog를 업데이트할 수 있다.  

```python
# Glue Data Catalog에 데이터베이스를 생성한다.
wr.catalog.create_database(name=...)

# Glue Data Catalog에 메타데이터 테이블을 생성한다.
wr.catalog.create_csv_table(
    table=...,
    column_types=...,
    ...
)
```

### [Amazon Athena](https://aws.amazon.com/ko/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc): Serverless Query Service depending on Glue

Athena는 interactive 쿼리 서비스로, Glue에서 정의한 data schema에 기반해 S3에 있는 데이터에 대해 쿼리를 실행한다. Athena는 Serverless로, 쿼리 실행을 위해 별도의 인프라를 설치할 필요가 없다! 또한 Glue에서와 마찬가지로 S3에 있는 데이터를 불러오거나 이동시키지 않고, 참조만 한다.

```python
# Athena S3 bucket을 생성한다.
wr.create_athena_bucket()

# Athena로 SQL 쿼리를 실행한다.
df = wr.athena.read_sql_query(sql=..., database=...)
```
위 파이썬 스크립트를 실행하면, Data Wrangler가 SQL 쿼리를 Athena로 전송한다. Athena는 쿼리를 실행하여 S3에 결과를 저장하고, Pandas DataFrame 형태로 반환한다. Athena는 open source SQL 분산 처리 엔진인 Presto에 기반하므로 자동으로 쿼리를 scale out하여  작은 쿼리로 분산 처리한다. 따라서 요구되는 연산 크기와 메모리 크기에 구애받지 않고 쿼리를 실행할 수 있다.


#### [정리]
- S3: AWS의 object storage 데이터 레이크
- Object storage: 다양한 포맷의 데이터를 저장하는 저장소
- Data Lake Governance: 데이터 레이크를 탐색하고 목록화하는 방법을 정의하고, 접근 권한을 제어하는 행위
- Data Wrangler: S3의 판다스 라이브러리
- Glue: S3의 데이터에 쿼리를 실행하기 위해 data schema를 정의하여 테이블에 저장하는 프로그램
- Athena: Glue의 정의를 기반으로 S3 데이터에 쿼리를 실행하는 SQL 분산 쿼리 서비스

