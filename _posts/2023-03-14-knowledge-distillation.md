---
title: "Knowledge Distillation, 2015"
categories: Papers
tags:
    - transfer learning
---


ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)ëŠ” í•™ìŠµëœ í¬ê¸°ê°€ í° ëª¨ë¸(ë“¤)ì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ í¬ê¸°ëŠ” ì‘ì€ ëª¨ë¸ë¡œ ì˜®ê¸°ëŠ” ë°©ë²•ì„ ë‹¤ë£¨ê³  ìˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ í•™ìŠµì„ ë§ˆì¹œ í›„ ë°°í¬í•˜ëŠ” ê³¼ì • ë“± ì—°ì‚° ì œì•½ì´ ìˆëŠ” ê²½ìš° ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆëŠ” ê¸°ìˆ ì´ë‹¤. 
{: .notice}

> Paper : [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)


## 1. Distillation

### 1.1 ê°œìš”

ì§€ì‹ ì¦ë¥˜ëŠ” ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” í°(cumbersome) ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•œ soft targetì„ í™œìš©í•´ ê·œëª¨ê°€ ì‘ì€(small) ëª¨ë¸ì— íš¨ê³¼ì ìœ¼ë¡œ transfer learningì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ë•Œ hard targetì´ë€  ë˜ëŠ” 1ë¡œ í‘œí˜„ë˜ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ true labelì„, soft targetì´ë€ softmax í•¨ìˆ˜ì˜ ê²°ê³¼ í™•ë¥  ë˜ëŠ” ë¡œì§“ì„ ì˜ë¯¸í•œë‹¤. Soft targetì€ ê·¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ê°’ì´ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ì´ëŸ° ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë” ì‘ì€ ëª¨ë¸ë¡œ ì´ì „í•˜ëŠ” ê²ƒì´ ì§€ì‹ ì¦ë¥˜ì˜ ëª©ì ì´ë‹¤. 

ì‹¤ì œë¡œëŠ” soft targetì— ëŒ€í•œ ì—¬ëŸ¬ í´ë˜ìŠ¤ì˜ softmax í™•ë¥  ê°’ì´ ë§¤ìš° ì‘ê¸° ë•Œë¬¸ì—, cross-entropy ë¹„ìš© í•¨ìˆ˜ì— íš¨ê³¼ì ìœ¼ë¡œ ì´ ì •ë³´ë¥¼ ì…ë ¥í•˜ê¸° ì–´ë µë‹¤. ì§€ì‹ ì¦ë¥˜ëŠ” ì´ ë¬¸ì œë¥¼ softmax í•¨ìˆ˜ì˜ ì˜¨ë„(temperature)ë¥¼ ë†’ì—¬ì„œ í•´ê²°í•œë‹¤. ì—¬ê¸°ì„œ ì˜¨ë„ë€ softmax í•¨ìˆ˜ì˜ ë³€ìˆ˜ë¡œ, ì˜¨ë„ë¥¼ ë†’ì¼ìˆ˜ë¡ í•¨ìˆ˜ëŠ” ë” ëœë¤í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•´ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ë¥¼ ì¤„ì¸ë‹¤. ë†’ì€ ì˜¨ë„ë¥¼ í†µí•´ ë” "ë¶€ë“œëŸ¬ìš´(soft)" í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê²°ë¡ ì ìœ¼ë¡œ soft targetë“¤ì˜ í™•ë¥ ì„ ì „ë°˜ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ ìˆë‹¤. 

ì§€ì‹ì¦ë¥˜ëŠ” í° ëª¨ë¸ì´ soft targetì„ ì¶©ë¶„íˆ ë„ì¶œí•  ë•Œê¹Œì§€ ì˜¨ë„ë¥¼ ì˜¬ë¦¬ê³ , ì´ ì˜¨ë„ë¥¼ ì‘ì€ ëª¨ë¸ì— ì ìš©í•´ soft targetì„ ì‘ì€ ëª¨ë¸ì— ë§¤ì¹˜ì‹œí‚¨ë‹¤.  ì´ë•Œ ì¦ë¥˜ëœ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” 1ï¸âƒ£ transfer setì— ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš°, í° ëª¨ë¸ì˜ soft targetê³¼ ë†’ì€ ì˜¨ë„ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. 2ï¸âƒ£ transfer setì˜ ë¼ë²¨ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ê²½ìš°, ì•ì„  í•™ìŠµê³¼ correct targetì„ 1ì˜ ì˜¨ë„ë¡œ í•™ìŠµí•œ cross-entropy í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ í‰ê· ì„ í™œìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì´ë‹¤. 

ì˜¨ë„ $T$ë¥¼ í¬í•¨í•œ softmax í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
q_i = \frac{exp(z_i/T)}{\sum_j exp(z_j)/T}
$$

2ï¸âƒ£ ì˜ ê²½ìš°ì— ëŒ€í•œ ìµœì¢… ì†ì‹¤ í•¨ìˆ˜ëŠ” ê°€ì¤‘ì§€ $\alpha$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤ ($T^2$ë¥¼ ìŠ¤ì¼€ì¼ë§ í•´ì£¼ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë•Œë¬¸ì¸ë°, ìì„¸íˆëŠ” ì•„ë˜ì—ì„œ ì‚´í´ë³¸ë‹¤):

$$
L = \alpha T^2 \text{CE} (P_{small}^{T=t}, P_{big}^{T=t}) + (1- \alpha) \text{CE}(P_{small}^{T=1}, \text{true label})
$$


#### ğŸ‰ ì˜ì‚¬ ì½”ë“œ

ìœ„ì—ì„œ ì •ì˜í•œ softmax í•¨ìˆ˜ì™€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ëµí•œ ì˜ì‚¬ ì½”ë“œë¥¼ ì“¸ ìˆ˜ ìˆë‹¤.

```python
# Define distilled model which has softmax layer with temperature as final layer.
class DistilledModel(Module):
    def __init__(self, input_dim, output_dim, temp=1, name=None):
        super().__init__(name=name)
        self.small_model = ... # define some small model.
        self.temp = temp
        
    def softmax_temp(self, x):
        exp_x = np.exp(x / self.temp)
        exp_x = exp_x - np.max(exp_x)
        sum_exp_x = np.sum(exp_x)
        return exp_x / sum_exp_x

    def __call__(self, x):
        logit = self.small_model(x) 
        output = self.softmax_temp(logit)
        return output
        
# Define final weighted cross-entropy function for distilled model.
# By the paper, it is recommended alpha > 0.5.
def final_cross_entropy(y_true, y_pred, y_soft, big_soft, alpha, high_temp):
    entropy_1 = CrossEntropy(y_soft, big_soft)
    entropy_2 = CrossEntropy(y_true, y_pred)
    return alpha * (high_temp ** 2) * entropy_1 + (1. - alpha) * entropy_2

# Train small model with soft target and high temperature from the cumbersome model.
distilled_model_1 = DistilledModel(input_dim, output_dim, temp=high_temp)
distilled_model_1.compile(...)
y_soft = distilled_model_1.fit((transfer_set, soft_target))

# Train true target of transfer set with temperature 1.
distilled_model_2 = DistilledModel(input_dim, output_dim)
distilled_model_2.compile(...)
y_pred = distilled_model_2.fit((transfer_set, true_target))

# Compute final cross-entropy function.
final_cross_entropy = final_cross_entropy(y_true, y_pred, 
                                          y_soft, big_soft,
                                          alpha, high_temp)
```


### 1.2 Logitê³¼ Distillation

ì‹¤ì œë¡œëŠ” Logitì„ í™œìš©í•˜ì—¬ cross-entropy í•¨ìˆ˜ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒ ë˜í•œ ì§€ì‹ ì¦ë¥˜ì˜ í•œ ì‚¬ë¡€ë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ë¡œì§“ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸ë¥¼ í•´ë³´ì.

- í° ëª¨ë¸ì˜ ë¡œì§“ $v_i$, í™•ë¥  $p_i$
- ì¦ë¥˜ëœ ëª¨ë¸ì˜ ë¡œì§“ $z_i$, í™•ë¥  $q_i$
- ì¦ë¥˜ëœ ëª¨ë¸ì˜ cross-entopy $C$

ì´ë•Œ ì¦ë¥˜ëœ ëª¨ë¸ì˜ cross-entropy ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤(ì—¬ê¸°ì„œëŠ” í•˜ë‚˜ì˜ ë¡œì§“ì— ëŒ€í•œ í¸ë¯¸ë¶„ìœ¼ë¡œ ë‚˜íƒ€ëƒˆë‹¤):

$$
\begin{aligned}
\frac{\partial C}{\partial z_i} 
&= \frac{1}{T} ( q_i - p_i) \\
&= \frac{1}{T} (\frac{e^{z_i/T}}{\sum_j e^{z_j/T}} - \frac{e^{v_i/T}}{\sum_j e^{v_j/T}})
\end{aligned}
$$

ì—¬ê¸°ì„œ 1ï¸âƒ£ ì˜¨ë„ê°€ ë¡œì§“ì˜ ê°’ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ í¬ë©´ (ì¦‰ $1/T$ê°€ ì „ì²´ê°’ì„ ì¶©ë¶„íˆ ì‘ê²Œ ë§Œë“¤ë©´) Smoothingì„ ì ìš©í•  ìˆ˜ ìˆë‹¤:

$$
\frac{\partial C}{\partial z_i} 
\approx 
\frac{1}{NT} (\frac{1 + e^{z_i/T}}{N + \sum_j e^{z_j/T}} - \frac{1 + e^{v_i/T}}{N + \sum_j e^{v_j/T}})
$$

ë˜í•œ 2ï¸âƒ£ ê°€ì •ì„ í†µí•´ ë‘ ë¡œì§“ì˜ í‰ê· ì´ $0$ì´ ë˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤ (ì¦‰ $\sum_j z_j = \sum_j v_j = 0 $). ìœ„ì˜ ë‘ê°€ì§€ ì¡°ê±´ì´ ë§Œì¡±ë˜ë©´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ **ë‘ ë¡œì§“ì˜ ì˜¤ì°¨ì— ëŒ€í•œ ìƒìˆ˜ë°°**ê°’ìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤:

$$
\frac{\partial C}{\partial z_i} 
\approx 
\frac{1}{NT^2} (z_i - v_i)
$$

ìœ„ì˜ í¸ë¯¸ë¶„ì„ ëª¨ë“  ë¡œì§“ $z_i$ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ êµ¬í•˜ê³  $T^2$ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©´, ì§€ì‹ ì¦ë¥˜ëŠ” ë‘ ë¡œì§“ì˜ ì˜¤ì°¨ ì œê³± $\frac{1}{2} (z_i - q_i)^2$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì‘ì—…ì´ ëœë‹¤.


